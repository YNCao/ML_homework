%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  Template for homework of Introduction to Machine Learning.
%
%  Fill in your name, lecture number, lecture date and body
%  of homework as indicated below.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[11pt,letter,notitlepage]{article}
%Mise en page
\usepackage[left=2cm, right=2cm, lines=45, top=0.8in, bottom=0.7in]{geometry}
\usepackage{fancyhdr}
\usepackage{fancybox}
\usepackage{graphicx}
\usepackage{pdfpages} 
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\headrulewidth}{1.5pt}
\renewcommand{\footrulewidth}{1.5pt}
\newcommand\Loadedframemethod{TikZ}
\usepackage[framemethod=\Loadedframemethod]{mdframed}

\usepackage{amssymb,amsmath}
\usepackage{amsthm}
\usepackage{thmtools}
\newtheorem{lemma}{Lemma}

\setlength{\topmargin}{0pt}
\setlength{\textheight}{9in}
\setlength{\headheight}{0pt}

\setlength{\oddsidemargin}{0.25in}
\setlength{\textwidth}{6in}

\usepackage{graphicx} % more modern
\usepackage{subfigure}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% Define math operator %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator*{\argmin}{\bf argmin}
\DeclareMathOperator*{\relint}{\bf relint\,}
\DeclareMathOperator*{\dom}{\bf dom\,}
\DeclareMathOperator*{\intp}{\bf int\,}
%%%%%%%%%%%%%%%%%%%%%%%


\setlength{\topmargin}{0pt}
\setlength{\textheight}{9in}
\setlength{\headheight}{0pt}

\setlength{\oddsidemargin}{0.25in}
\setlength{\textwidth}{6in}
\pagestyle{fancy}
%%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%%
\mdtheorem[
topline=false,
rightline=false,
leftline=false,
bottomline=false,
leftmargin=-10,
rightmargin=-10
]{exercise}{\textbf{Exercise}}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Problem environment %%
%%%%%%%%%%%%%%%%%%%%%%%%
\mdtheorem[
topline=false,
rightline=false,
leftline=false,
bottomline=false,
leftmargin=-10,
rightmargin=-10
]{problem}{\textbf{Problem}}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Solution Environment %%
%%%%%%%%%%%%%%%%%%%%%%%
\declaretheoremstyle
[
spaceabove=0pt, 
spacebelow=0pt, 
headfont=\normalfont\bfseries,
notefont=\mdseries, 
notebraces={(}{)}, 
headpunct={:\quad}, 
headindent={},
postheadspace={ }, 
postheadspace=4pt, 
bodyfont=\normalfont, 
qed=$\blacksquare$,
preheadhook={\begin{mdframed}[style=myframedstyle]},
	postfoothook=\end{mdframed},
]{mystyle}

\declaretheorem[style=mystyle,title=Solution,numbered=no]{solution}
\mdfdefinestyle{myframedstyle}{%
	topline=false,
	rightline=false,
	leftline=false,
	bottomline=false,
	skipabove=-6ex,
	leftmargin=-10,
	rightmargin=-10}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Solution environment %%
%%%%%%%%%%%%%%%%%%%%%%%

%% Homework info.
\newcommand{\posted}{\text{Apr. 5, 2020}}       			%%% FILL IN POST DATE HERE
\newcommand{\due}{\text{Apr. 12, 2020}} 			%%% FILL IN Due DATE HERE
\newcommand{\hwno}{\text{4}} 		           			%%% FILL IN LECTURE NUMBER HERE


%%%%%%%%%%%%%%%%%%%%
%% Put your information here %%
%%%%%%%%%%%%%%%%%%%
\newcommand{\name}{\text{Yunning Cao}}  	          			%%% FILL IN YOUR NAME HERE
\newcommand{\id}{\text{PB16021370}}		       			%%% FILL IN YOUR ID HERE
%%%%%%%%%%%%%%%%%%%%
%% End of the student's info %%
%%%%%%%%%%%%%%%%%%%


\newcommand{\proj}[2]{\textbf{P}_{#2} (#1)}
\newcommand{\lspan}[1]{\textbf{span}  (#1)  }
\newcommand{\rank}[1]{ \textbf{rank}  (#1)  }
\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}


\lhead{
	\textbf{\name}
}
\rhead{
	\textbf{\id}
}
\chead{\textbf{
		Homework \hwno
}}


\begin{document}
\vspace*{-4\baselineskip}
\thispagestyle{empty}


\begin{center}
{\bf\large Introduction to Machine Learning}\\
{Spring 2020}\\
University of Science and Technology of China
\end{center}

\noindent
Lecturer: Jie Wang  			 %%% FILL IN LECTURER HERE
\hfill
Homework \hwno             			
\\
Posted: \posted
\hfill
Due: \due
\\
Name: \name             			
\hfill
ID: \id						
\hfill

\noindent
\rule{\textwidth}{2pt}

\medskip





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BODY OF HOMEWORK GOES HERE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Notice, }to get the full credits, please show your solutions step by step.

\begin{exercise}[Logistic Regression \textnormal{40pts}]
Given the training data $\mathcal{D}=\{ (\textbf{x}_i,y_i) \}_{i=1}^n$, where $\textbf{x}_i \in \mathbb{R}^d$ and $y_i \in \{ 0,1 \}$. Let
\begin{align*}
    \mathcal{I}^+&=\{i:i\in[n],y_i=1\},\\
    \mathcal{I}^-&=\{i:i\in[n],y_i=0\},
\end{align*}
where $[n]=\{1,2,\ldots,n\}$. We assume that $\mathcal{I}^+$ and $\mathcal{I}^-$ are not empty.

Then, we can formulate the logistic regression as:
	\begin{equation}\label{prob:logistic}
	\min_{\textbf{w}}\,\,L(\textbf{w})=-\frac{1}{n}\sum_{i=1}^n \left( y_i \log \left( \frac{\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)}{1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle) } \right) + (1-y_i)\log \left( \frac{1}{1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)} \right) \right),
	\end{equation}
	where $\mathbf{w} \in \mathbb{R}^{d+1}$ is the model parameter to be estimated and $ \overline{\mathbf{x}}_i^{\top} = (1,\mathbf{x}_i^{\top}) $.
	
	

    \begin{enumerate}
    	\item (5pts) Find the gradient and the Hessian of $L(\textbf{w})$.
    	\item (10pts) Suppose that $\overline{\textbf{X}}=(\overline{\mathbf{x}}_1,\overline{\mathbf{x}}_2,\dots,\overline{\mathbf{x}}_n)^\top\in\mathbb{R}^{n \times (d+1)}$ and $\rank{\overline{\mathbf{X}}}=d+1$. Show that $L(\textbf{w})$ is strictly convex, i.e., for all $\textbf{w}_1\neq \textbf{w}_2$,
    	\begin{align*}
    	    L(t\textbf{w}_1 + (1-t)\textbf{w}_2) < t L(\textbf{w}_1)+(1-t)L(\textbf{w}_2),\forall\, t \in (0,1).
    	\end{align*}
    	\item (10pts) Suppose that the training data is strictly linearly separable, that is, there exists $\hat{\mathbf{w}}\in\mathbb{R}^{d+1}$ such that
    	\begin{align*}
    	    &\langle \hat{\mathbf{w}}, \mathbf{\bar{x}}_i\rangle>0,\,\forall\,i\in\mathcal{I}^+,\\
    	    &\langle \hat{\mathbf{w}}, \mathbf{\bar{x}}_i\rangle<0,\,\forall\,i\in\mathcal{I}^-.
    	\end{align*}
    	Show that problem (\ref{prob:logistic}) has no solution.
    \end{enumerate}
	Let $\overline{\mathbf{z}}_i = (2 y_i-1)\overline{\mathbf{x}}_i$. 
    \begin{enumerate}[resume]
        \item (5pts) Show that
        \begin{align*}
            L(\textbf{w}) = \frac{1}{n} \sum_{i=1}^n \log(1+\exp(-\langle \mathbf{w},\overline{\mathbf{z}}_i \rangle)).
        \end{align*}
    	\item (20pts) Suppose that the training data is NOT linearly separable. That is, for all $\mathbf{w} \in \mathbb{R}^{d+1}$, there exists $ i \in \left[ n \right] $ such that
    	\begin{align*}
    	    \langle \mathbf{w}, \mathbf{\bar{z}}_i\rangle<0.
    	\end{align*}
    	Show that problem (\ref{prob:logistic}) always admits a solution.
    \end{enumerate}
\end{exercise}

\begin{solution}

\end{solution}

\newpage
\begin{exercise}[Programming Exercise: Naive Bayes  \textnormal{20pts}]
We provide you with a data set that contains spam and non-spam emails (``hw4\_nb.zip"). Please use the Naive Bayes Classifier to detect the spam emails.
Finish the following exercises by programming. You can use your favorite programming language.
\begin{enumerate}
\item Remove all the tokens that contain non-alphabetic characters.
\item Train the Naive Bayes Classifier on the training set according to Algorithm \ref{alg:train_bayes}.
\item Test the Naive Bayes Classifier on the test set according to Algorithm \ref{alg:test_bayes}.
\item Compute the confusion matrix, precision, recall, and F1 score. Please report your result.
\end{enumerate}

\end{exercise}

\begin{algorithm}
\caption{Training Naive Bayes Classifier}
\label{alg:train_bayes}
\textbf{Input:} The training set with the labels $\mathcal{D}=\{(\mathbf{x}_i,y_i)\}.$
\begin{algorithmic}[1]
\STATE $\mathcal{V}\leftarrow$ the set of distinct words and other tokens found in $\mathcal{D}$\\
\FOR{each target value $c$ in the labels set $\mathcal{C}$} 
\STATE $\mathcal{D}_c\leftarrow$ the training samples whose labels are $c$\\
\STATE $P(c)\leftarrow\frac{|\mathcal{D}_c|}{|\mathcal{D}|}$\\
\STATE $T_c\leftarrow$ a single document by concatenating all training samples in $\mathcal{D}_c$\\
\STATE $n_c\leftarrow |T_c|$
\FOR{each word $w_k$ in the vocabulary $\mathcal{V}$}
\STATE $n_{c,k}\leftarrow$ the number of times the word $w_k$ occurs in $T_c$\\
\STATE $P(w_k|c)=\frac{n_{c,k}+1}{n_c+|\mathcal{V}|}$
\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Testing Naive Bayes Classifier }
\label{alg:test_bayes}
\textbf{Input:} An email $\mathbf{x}$. Let $x_i$ be the $i^{th}$ token in $\mathbf{x}$ . $\mathcal{I}=\emptyset.$
\begin{algorithmic}[1]
\FOR{$i=1,\dots,|\mathbf{x}|$} 
\IF{$\exists\, w_{k_i}\in\mathcal{V}$ such that $w_{k_i}=x_i$}
\STATE $\mathcal{I}\leftarrow\mathcal{I}\cup k_i$
\ENDIF
\ENDFOR
\STATE predict the label of $\mathbf{x}$ by 
\begin{align*}
    \hat{y}=\arg\max_{c\in\mathcal{C}} P(c)\prod_{i\in\mathcal{I}}P(w_{k_i}|c)
\end{align*}
\end{algorithmic}
\end{algorithm}


\begin{solution}
   
\end{solution}

\newpage


\begin{exercise} [Example in Stochastic Gradient Descent \textnormal{15pts}]
Consider a simple linear regression model $ f(x;w) = wx $ with samples $ \{(x_i,y_i)\}_{i=1}^2 $, where $x_i,y_i \in \mathbb{R}$ for $i=1,2$. We use SGD algorithm to minimize the average fitting error 
\begin{align*}
L(w) = \frac{1}{2}\sum_{i=1}^{2}(y_i-wx_i)^2.
\end{align*}
We uniformly sample a data instance $ \xi_k=(x_{i_k}, y_{i_k}) $ from $ \{(x_i,y_i)\}_{i=1}^2 $ at $ k^{th} $ iteration. The sequence $(w_k)$ is generated by the stochastic gradient descent algorithm.
\begin{enumerate}
	\item (5pts)   Please write down the derivative $  L^\prime(w_k) $ and stochastic derivative $ g_k $.
	\item (5pts) Please write down the variance of the stochastic derivative $ \mathbb{V}_{\xi_k}[g_k] $.
	\item (5pts) We assume the upper bound of $ \mathbb{V}_{\xi_k}[g_k] $ takes the form of
	\begin{align*}
	\mathbb{V}_{\xi_k}[g_k]\leq M + M_V |L^\prime(w_k)|^2.
	\end{align*}
	Please find the corresponding $ M $ and $ M_V $ in this problem. Specifically, when will $ M $ become zero, and when will $ M_V $ become zero?
\end{enumerate}
\end{exercise}
\begin{solution}

\end{solution}
\newpage


\begin{exercise}[Convergence of SGD for Convex Function \textnormal{25pts}]
	Suppose $f:\mathbb{R}^n \rightarrow \mathbb{R}$ is convex and continuously differentiable, and it attains its minimum at $\mathbf{x}^*$. Suppose the second moment of stochastic gradient $ \mathbf{g} $ is bounded, i.e.,
	\begin{align*}
	\mathbb{E}_{\xi}[\|\mathbf{g}(\mathbf{x},\xi)\|_2^2]\leq G^2,\,\forall\,\mathbf{x}\in \mathbb{R}^n.
	\end{align*}
	Suppose $(\mathbf{x}_k)$ is a sequence generated by SGD algorithm with a fixed stepsize $\alpha$. Define 
	$ \mathbf{\tilde{x}}_K=\frac{1}{K}\sum_{k=1}^{K} \mathbf{x}_k $ and $ f^*=f(\mathbf{x}^*) $. 
    
	
	\begin{enumerate}
	    \item Let $X, Y,$ and $Z$ be random variables.
	    \begin{enumerate}
	        \item (5pts) Show that the tower property holds, i.e.,
        	\begin{align*}
        	    \mathbb{E}[X|Y] = \mathbb{E}[\mathbb{E}[X|Y,Z] |Y].
        	\end{align*}
        	\item (5pts) The variance decomposition formula holds, i.e.,
        	\begin{align*}
        	    \mathbb{V}[X] = \mathbb{E}[\mathbb{V}[X|Y]]+\mathbb{V}[\mathbb{E}[X|Y]].
        	\end{align*}
	    \end{enumerate}
    	\emph{Hint: if you do not know measure theory well, you can assume that $X$, $Y$, and $Z$ are continuous random variables.}
		\item (5pts) Suppose the stochastic gradient at $ k^{th} $ iteration is $ \mathbf{g}_k $.  Please show that
		\begin{align*}
		\mathbb{E}_{\xi_1:\xi_k}[f(\mathbf{x}_k)-f^*]\leq\mathbb{E}_{\xi_1:\xi_k}[\langle{\mathbf{g}_k, \mathbf{x}_k-\mathbf{x}^*}\rangle].
		\end{align*}
		\item (5pts) Please show that
		\begin{align}
		\mathbb{E}_{\xi_1:\xi_k}[f(\mathbf{x}_k)-f^*]\leq\frac{1}{2\alpha}\mathbb{E}_{\xi_1:\xi_k}[\|\mathbf{x}_k-\mathbf{x}^*\|_2^2 - \|\mathbf{x}_{k+1}-\mathbf{x}^*\|_2^2 + \alpha^2 \|\mathbf{g}_k\|_2^2].
		\end{align}
		\item (5pts) Please show that
		\begin{align*}
		\mathbb{E}_{\xi_1:\xi_K}[f(\tilde{x}_K)-f^*] &\leq \frac{\|x_1-x^*\|_2^2 + \alpha^2 G^2 K}{2\alpha K}\\
		&\xrightarrow[]{O(1/K)}\frac{\alpha G^2}{2}.
		\end{align*}
	\end{enumerate}
\end{exercise}
\begin{solution}

\end{solution}
\newpage


\begin{exercise}[Programming Exercise: Logistic Regression \textnormal{20pts}]

We provide you with a dataset of handwritten digits\footnotemark\,that contains a training set of 60000 examples and a test set of 1960 examples (``hw4\_lr.zip''). Each image in this dataset  has $28\times28$ pixels and the associated label is the handwritten digit---that is, an integer from the set $\{0,1,\cdots,9\}$---in the image. In this exercise, you need to build a logistic regression classifier to predict if a given image has the handwritten digit $0$ in it or not. You can use your favorite programming language to finish this exercise.
\begin{enumerate}
\item
\begin{enumerate}
    \item Choose a proper normalization method to process the data matrix. Please report the normalization method you use.
    \item Find a Lipschitz constant of $\nabla L(\mathbf{w})$, where $L(\mathbf{w})$ is the objective function of the logistic regression after normalizing and  $\mathbf{w}$ is the model parameter to be estimated. Please report your result.
\end{enumerate}
\item
\begin{enumerate}
    \item Use GD and SGD to train the logistic regression classifier on the training set, respectively. Evaluate the classification accuracy on the training set after each iteration. Stop the iteration when $\text{Accuracy}\geq 97\%$. Please plot the accuracy of these two classifiers (the one trained by GD and the other trained by SGD) versus the iteration step on one graph.
    \item Compare the total iteration counts and the total time cost of the two methods (GD and SGD), respectively. Please report your result.
    \item Compare the confusion matrix, precision, recall and F1 score of the two classifiers (the one trained by GD and the other trained by SGD). Please report your result.
\end{enumerate}
\item 
\begin{enumerate}
    \item The training set is imbalanced as the majority class has roughly ten times more images than the minority class. Imbalanced data can hurt the performance of the classifiers badly. Thus, please undersample the majority class such that the numbers of images in the two classes are roughly the same. 
    \item Use GD to train the logistic regression classifier on the new training set after undersampling. Stop the iteration when $\text{Accuracy}\geq 97\%$.
    \item Evaluate the two classifiers (the one trained with GD on the original training set and the other trained on the new training set after undersampling) on the test set. Compare the confusion matrix, precision, recall and F1 score of the two classifiers. Please report your result.
\end{enumerate}

\end{enumerate}
\end{exercise}
\footnotetext[1]{This dataset is modified from the MNITS dataset: http://yann.lecun.com/exdb/mnist/}
\begin{solution}
\end{solution}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
